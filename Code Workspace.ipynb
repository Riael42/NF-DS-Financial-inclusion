{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abandon all hope ye who enter here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mass model trainer\n",
    "\n",
    "def model_trainer(X_train, y_train, X_test, y_test, random_seed):\n",
    "    stratify = 0 #switch whether or not to stratify target_variable\n",
    "    target_variable = \"bank_account\" #maybe we use this for the next project too\n",
    "    niter = 100 #number iterators\n",
    "    num_cv = 5 #num cross validation folds\n",
    "    verbose=0 #quiet 0 1 2 loud\n",
    "    num_jobs = -1 #-1 all cpu cores, 1 disables parallelization, 2 uses specified number\n",
    "    scoring = \"f1\" #if we want to change scoring for whatever reason\n",
    "    #todo: random_train_split make it so you randomize splits between models\n",
    "\n",
    " \n",
    "    #not random parameters for logistic\n",
    "    target_variable = \"bank_account\" #maybe we use this for the next project too\n",
    "    niter_logi = 50 #number iterators\n",
    "    num_cv_logi = 20 #num cross validation folds\n",
    "    verbose=0 #quiet 0 1 2 loud\n",
    "    num_jobs = -1 #-1 all cpu cores, 1 disables parallelization, 2 uses specified number\n",
    "    scoring_logi = \"f1\" #if we want to change scoring for whatever reason\n",
    "    random_seed = 42\n",
    "    penaltea = ['l2', 'l1'] #penalty for logistic regression\n",
    "    C_log = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] #C for logistic regression\n",
    "    slogver = [ 'SGD', 'liblinear'] #solver for logistic regression\n",
    "    miter = [120, 130, 135, 140, 145, 150] #max iterations for logistic regression\n",
    "    logi_weights = { 0 : 0.1, 1: 0.9}\n",
    "\n",
    "    #not random parameters for random forest\n",
    "    niter_forest = 100\n",
    "    forest_cv = 15\n",
    "    scoring_forest = \"f1\"\n",
    "    nestimators = [10, 25, 55] # num estimators for random forest\n",
    "    big_deep = [15, 25, 30] #maximum depth for random forest\n",
    "    min_forest_split = [4, 5, 6, 10, 12] # min_samples split\n",
    "    min_tea_leafs = [3, 4, 5, 10, 15] #min_samples_leafs\n",
    "    bootstrap = [True, False] # boostrap\n",
    "    cryterio = ['gini', 'entropy'] # criterion\n",
    "    forest_weights = { 0 : 0.5, 1: 0.5}\n",
    "\n",
    "    #not random params for random xgboost\n",
    "    xbgeta = [0.2, 0.3, 0.5, 0.75, 1] #eta\n",
    "    XgammaBOOST = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  #gamma \n",
    "    deep_boost = [5, 6, 7, 8] #max_depth\n",
    "    min_boost_weight = [2, 3, 4, 5] #min_child_weight\n",
    "    xgboosamplet = [0.5, 0.6, 0.7, 0.8, 0.9] #subsample\n",
    "    xgbytree = [0.6, 0.8, 1.0] #colsample_bytree\n",
    "    xgboojective = [\"binary:logistic\"] #objective\n",
    "    xgauc = [\"auc\"] #eval_metric\n",
    "    xgscale = [1.2, 1.3] #scale_pos_weight\n",
    "    xgbniter = 50\n",
    "    xgbcv = 15\n",
    "    scoring_xgb = \"roc_auc\"\n",
    "    \n",
    "    #not random params for svm\n",
    "    svm_c = [0.1, 1, 10, 100]\n",
    "    svm_kernel = [\"linear\", \"rbf\", \"poly\"]\n",
    "    svm_gamma = [\"scale\", \"auto\"]\n",
    "    svm_degree = [3, 4, 5]\n",
    "    svc_niter = 10\n",
    "    svc_cv = 5\n",
    "    svc_weight = {0 : 1, 1: 5}\n",
    "\n",
    "    #voting classifier\n",
    "    voting = \"hard\" #can be soft\n",
    "\n",
    "\n",
    "    #print(\"shape: \", target.shape)\n",
    "    #print(\"unique values: \", target.unique())\n",
    "    #print(train.dtypes)\n",
    "    #print(target.dtypes)\n",
    "    #print(target)\n",
    "\n",
    "    #Darth Loger\n",
    "    #train logistic\n",
    "    #high recall, low precision\n",
    "    tuned_logistic = LogisticRegression(class_weight=logi_weights)\n",
    "\n",
    "    params = {\n",
    "        'penalty': penaltea, \n",
    "        'C': C_log,\n",
    "        'solver': slogver,\n",
    "        'max_iter': miter\n",
    "    }\n",
    "    tuned_logistic = RandomizedSearchCV(estimator=tuned_logistic, \n",
    "                                        param_distributions=params, \n",
    "                                        n_iter=niter_logi, cv=num_cv_logi, \n",
    "                                        random_state=random_seed, verbose=verbose,\n",
    "                                        n_jobs = num_jobs,\n",
    "                                        scoring=scoring_logi)\n",
    "    tuned_logistic.fit(X_train, y_train)\n",
    "    print(tuned_logistic.best_params_)\n",
    "    tuned_logistic = tuned_logistic.best_estimator_\n",
    "\n",
    "    #Darth Lorax\n",
    "    #train random forest\n",
    "    #0.7 precision 0.3 recall\n",
    "    tuned_forest = RandomForestClassifier(class_weight=forest_weights)\n",
    "\n",
    "    params = {\n",
    "    'n_estimators': nestimators, \n",
    "    'max_depth': big_deep,\n",
    "    'min_samples_split': min_forest_split,\n",
    "    'min_samples_leaf': min_tea_leafs,\n",
    "    'bootstrap': bootstrap,\n",
    "    'criterion': cryterio\n",
    "    }\n",
    "\n",
    "    tuned_forest = RandomizedSearchCV(estimator=tuned_forest,\n",
    "                                        param_distributions=params,\n",
    "                                        n_iter=niter_forest, cv=forest_cv,\n",
    "                                        random_state=random_seed, verbose=verbose,\n",
    "                                        n_jobs = num_jobs,\n",
    "                                        scoring=scoring_forest)\n",
    "    tuned_forest.fit(X_train, y_train)\n",
    "    print(tuned_forest.best_params_)\n",
    "    tuned_forest = tuned_forest.best_estimator_\n",
    "\n",
    "    #Darth XGBious\n",
    "    #train xgboost\n",
    "    # 0.66 precision 0.33 recall\n",
    "    tuned_xgboost = xgb.XGBClassifier()\n",
    "\n",
    "    params = {\n",
    "        \"eta\" : xbgeta,\n",
    "        \"gamma\": XgammaBOOST,\n",
    "        \"max_depth\" : deep_boost,\n",
    "        \"min_child_weight\" : min_boost_weight,\n",
    "        \"subsample\" : xgboosamplet,\n",
    "        \"colsample_bytree\" : xgbytree,\n",
    "        \"objective\" : xgboojective,\n",
    "        \"eval_metric\" : xgauc,\n",
    "        \"scale_pos_weight\" : xgscale\n",
    "    }\n",
    "    tuned_xgboost = RandomizedSearchCV(estimator=tuned_xgboost,\n",
    "                                        param_distributions=params,\n",
    "                                        n_iter=xgbniter, cv=xgbcv,\n",
    "                                        random_state=random_seed, verbose=verbose,\n",
    "                                        n_jobs = num_jobs,\n",
    "                                        scoring=scoring_xgb)\n",
    "    tuned_xgboost.fit(X_train, y_train)\n",
    "    print(tuned_xgboost.best_params_)\n",
    "    tuned_xgboost = tuned_xgboost.best_estimator_\n",
    "\n",
    "\n",
    "    #SVM\n",
    "    \n",
    "    params = {\n",
    "        \"C\" : svm_c,\n",
    "        \"kernel\" : svm_kernel,\n",
    "        \"gamma\" : svm_gamma,\n",
    "        \"degree\" : svm_degree\n",
    "    }\n",
    "    tuned_svc = SVC(class_weight = svc_weight)\n",
    "    tuned_svc = RandomizedSearchCV( estimator=tuned_svc,\n",
    "                                    param_distributions=params,\n",
    "                                    n_iter = svc_niter,\n",
    "                                    cv = svc_cv,\n",
    "                                    random_state=random_seed\n",
    "    )\n",
    "    tuned_svc.fit(X_train, y_train)\n",
    "    print(tuned_svc.best_params_)\n",
    "    tuned_svc = tuned_svc.best_estimator_\n",
    "\n",
    "    #classifier\n",
    "    sith_council = VotingClassifier(estimators = [\n",
    "                                    (\"logistic\", tuned_logistic),\n",
    "                                    (\"random_forest\", tuned_forest),\n",
    "                                    (\"xgboost\", tuned_xgboost),\n",
    "                                    (\"svm\", tuned_svc)], voting=voting\n",
    "    )\n",
    "    sith_council.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    #The great wall of prints\n",
    "\n",
    "    print(\"----------\")\n",
    "    print(\"Logistic Regression\")\n",
    "    print(\"----------\")\n",
    "    print(\"Classification report on train data:\")\n",
    "    print(classification_report(y_train, tuned_logistic.predict(X_train)))\n",
    "    print(\"----------\")\n",
    "    print(\"Classification report on test data:\")\n",
    "    print(classification_report(y_test, tuned_logistic.predict(X_test)))\n",
    "    print(\"----------\")\n",
    "    print(\"Random Forest\")\n",
    "    print(\"----------\")\n",
    "    print(\"Classification report on train data:\")\n",
    "    print(classification_report(y_train, tuned_forest.predict(X_train)))\n",
    "    print(\"----------\")\n",
    "    print(\"Classification report on test data:\")\n",
    "    print(classification_report(y_test, tuned_forest.predict(X_test)))\n",
    "    print(\"----------\")\n",
    "    print(\"XGBoost\")\n",
    "    print(\"----------\")\n",
    "    print(\"Classification report on train data:\")\n",
    "    print(classification_report(y_train, tuned_xgboost.predict(X_train)))\n",
    "    print(\"----------\")\n",
    "    print(\"Classification report on test data:\")\n",
    "    print(classification_report(y_test, tuned_xgboost.predict(X_test)))\n",
    "    print(\"----------\")\n",
    "    print(\"SVM\")\n",
    "    print(\"----------\")\n",
    "    print(\"Classification report on train data:\")\n",
    "    print(classification_report(y_train, tuned_svc.predict(X_train)))\n",
    "    print(\"----------\")\n",
    "    print(\"Classification report on test data:\")\n",
    "    print(classification_report(y_test, tuned_svc.predict(X_test)))\n",
    "    print(\"----------\")\n",
    "    print(\"Voting Classifier\")\n",
    "    print(\"----------\")\n",
    "    print(\"Classification report on train data:\")\n",
    "    print(classification_report(y_train, sith_council.predict(X_train)))\n",
    "    print(\"----------\")\n",
    "    print(\"Classification report on test data:\")\n",
    "    print(classification_report(y_test, sith_council.predict(X_test)))\n",
    "    print(\"----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mess with params for logi \n",
    "\n",
    "target_variable = \"bank_account\" #maybe we use this for the next project too\n",
    "niter_logi = 50 #number iterators\n",
    "num_cv_logi = 20 #num cross validation folds\n",
    "verbose=0 #quiet 0 1 2 loud\n",
    "num_jobs = -1 #-1 all cpu cores, 1 disables parallelization, 2 uses specified number\n",
    "scoring_logi = \"f1_macro\" #if we want to change scoring for whatever reason\n",
    "random_seed = 42\n",
    "penaltea = ['l2', 'l1'] #penalty for logistic regression\n",
    "C_log = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] #C for logistic regression\n",
    "slogver = [ 'SGD', 'liblinear'] #solver for logistic regression\n",
    "miter = [120, 130, 135, 140, 145, 150] #max iterations for logistic regression\n",
    "logi_weights = { 0 : 0.4, 1: 0.6}\n",
    "\n",
    "tuned_logistic = LogisticRegression(class_weight=logi_weights)\n",
    "\n",
    "params = {\n",
    "    'penalty': penaltea, \n",
    "    'C': C_log,\n",
    "    'solver': slogver,\n",
    "    'max_iter': miter\n",
    "}\n",
    "\n",
    "tuned_logistic = RandomizedSearchCV(estimator=tuned_logistic, \n",
    "                                    param_distributions=params, \n",
    "                                    n_iter=niter_logi, cv=num_cv_logi, \n",
    "                                    random_state=random_seed, verbose=verbose,\n",
    "                                    n_jobs = num_jobs,\n",
    "                                    scoring=scoring_logi)\n",
    "tuned_logistic.fit(X_train_balanced, y_train_balanced)\n",
    "best_params = tuned_logistic.best_params_\n",
    "tuned_logistic = tuned_logistic.best_estimator_\n",
    "print(\"----------\")\n",
    "print(\"Logistic Regression\")\n",
    "print(\"----------\")\n",
    "print(\"Classification report on train data:\")\n",
    "print(classification_report(y_train_balanced, tuned_logistic.predict(X_train_balanced)))\n",
    "print(\"----------\")\n",
    "print(\"----------\")\n",
    "print(\"Classification report on imba train data:\")\n",
    "print(classification_report(y_train, tuned_logistic.predict(X_train)))\n",
    "print(\"----------\")\n",
    "print(\"Classification report on test data:\")\n",
    "print(classification_report(y_test, tuned_logistic.predict(X_test)))\n",
    "print(\"----------\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mess with params for forest\n",
    "\n",
    "#not random parameters for random forest\n",
    "niter_forest = 100\n",
    "forest_cv = 15\n",
    "scoring_forest = \"f1\"\n",
    "nestimators = [50, 100, 150, 200, 250, 300] # num estimators for random forest\n",
    "big_deep = [8, 9, 10, 11, 12, 13, 14, 15] #maximum depth for random forest\n",
    "min_forest_split = [3, 4, 5, 6, 10, 12] # min_samples split\n",
    "min_tea_leafs = [3, 4, 5, 10, 15] #min_samples_leafs\n",
    "bootstrap = [True, False] # boostrap\n",
    "cryterio = ['gini', 'entropy'] # criterion\n",
    "forest_weights = { 0 : 0.6, 1: 0.4}\n",
    "\n",
    "tuned_forest = RandomForestClassifier(class_weight=forest_weights)\n",
    "\n",
    "params = {\n",
    "'n_estimators': nestimators, \n",
    "'max_depth': big_deep,\n",
    "'min_samples_split': min_forest_split,\n",
    "'min_samples_leaf': min_tea_leafs,\n",
    "'bootstrap': bootstrap,\n",
    "'criterion': cryterio\n",
    "}\n",
    "\n",
    "tuned_forest = RandomizedSearchCV(estimator=tuned_forest,\n",
    "                                    param_distributions=params,\n",
    "                                    n_iter=niter_forest, cv=forest_cv,\n",
    "                                    random_state=random_seed, verbose=verbose,\n",
    "                                    n_jobs = num_jobs,\n",
    "                                    scoring=scoring_forest)\n",
    "tuned_forest.fit(X_train_balanced, y_train_balanced)\n",
    "best_params = tuned_forest.best_params_\n",
    "tuned_forest = tuned_forest.best_estimator_\n",
    "print(\"----------\")\n",
    "print(\"Forest Random\")\n",
    "print(\"----------\")\n",
    "print(\"Classification report on train data:\")\n",
    "print(classification_report(y_train_balanced, tuned_forest.predict(X_train_balanced)))\n",
    "print(\"----------\")\n",
    "print(\"Classification report on imba train data:\")\n",
    "print(classification_report(y_train, tuned_forest.predict(X_train)))\n",
    "print(\"----------\")\n",
    "print(\"Classification report on test data:\")\n",
    "print(classification_report(y_test, tuned_forest.predict(X_test)))\n",
    "print(\"----------\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not random params for random xgboost\n",
    "xbgeta = [0.05, 0.1, 0.15] #eta\n",
    "XgammaBOOST = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  #gamma \n",
    "xgb_num_boost= [50, 100] #num iter\n",
    "deep_boost = [3, 4, 5, 6, 7, 8] #max_depth\n",
    "min_boost_weight = [2, 3, 4, 5] #min_child_weight\n",
    "xgboosamplet = [0.5, 0.6, 0.7, 0.8, 0.9] #subsample\n",
    "xgbytree = [0.6, 0.8, 1.0] #colsample_bytree\n",
    "xgboojective = [\"binary:logistic\"] #objective\n",
    "xgauc = [\"auc\"] #eval_metric\n",
    "xgscale = [1.0] #scale_pos_weight\n",
    "xgbcv = 15\n",
    "scoring_xgb = \"roc_auc\"\n",
    "\n",
    "#mess with params for xgboost\n",
    "tuned_xgboost = xgb.XGBClassifier()\n",
    "\n",
    "params = {\n",
    "    \"eta\" : xbgeta,\n",
    "    \"gamma\": XgammaBOOST,\n",
    "    \"max_depth\" : deep_boost,\n",
    "    \"min_child_weight\" : min_boost_weight,\n",
    "    \"subsample\" : xgboosamplet,\n",
    "    \"colsample_bytree\" : xgbytree,\n",
    "    \"objective\" : xgboojective,\n",
    "    \"eval_metric\" : xgauc,\n",
    "    \"scale_pos_weight\" : xgscale,\n",
    "    'n_estimators' : xgb_num_boost\n",
    "}\n",
    "tuned_xgboost = RandomizedSearchCV(estimator=tuned_xgboost,\n",
    "                                    param_distributions=params, cv=xgbcv,\n",
    "                                    random_state=random_seed, verbose=verbose,\n",
    "                                    n_jobs = num_jobs,\n",
    "                                    scoring=scoring_xgb)\n",
    "tuned_xgboost.fit(X_train_balanced, y_train_balanced)\n",
    "best_params = tuned_xgboost.best_params_\n",
    "tuned_xgboost = tuned_xgboost.best_estimator_\n",
    "print(\"----------\")\n",
    "print(\"gxboost\")\n",
    "print(\"----------\")\n",
    "print(\"Classification report on train data:\")\n",
    "print(classification_report(y_train_balanced, tuned_xgboost.predict(X_train_balanced)))\n",
    "print(\"----------\")\n",
    "print(\"Classification report on imba data:\")\n",
    "print(classification_report(y_train, tuned_xgboost.predict(X_train)))\n",
    "print(\"----------\")\n",
    "print(\"Classification report on test data:\")\n",
    "print(classification_report(y_test, tuned_xgboost.predict(X_test)))\n",
    "print(\"----------\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mess with svm\n",
    "svm_c = [0.1, 0.5, 0.75, 1, 2, 3, 4]\n",
    "svm_kernel = [\"linear\", \"rbf\"]\n",
    "svm_gamma = [\"scale\", \"auto\"]\n",
    "svc_niter = 10\n",
    "svc_cv = 15\n",
    "svc_weight = {0 : 0.9, 1: 1.3}\n",
    "\n",
    "params = {\n",
    "    \"C\" : svm_c,\n",
    "    \"kernel\" : svm_kernel,\n",
    "    \"gamma\" : svm_gamma\n",
    "}\n",
    "tuned_svc = SVC(class_weight = svc_weight)\n",
    "tuned_svc = RandomizedSearchCV( estimator=tuned_svc,\n",
    "                                param_distributions=params,\n",
    "                                n_iter = svc_niter,\n",
    "                                cv = svc_cv,\n",
    "                                random_state=random_seed\n",
    ")\n",
    "tuned_svc.fit(X_train_balanced, y_train_balanced)\n",
    "best_params = tuned_svc.best_params_\n",
    "tuned_svc = tuned_svc.best_estimator_\n",
    "print(\"----------\")\n",
    "print(\"svn\")\n",
    "print(\"----------\")\n",
    "print(\"Classification report on train data:\")\n",
    "print(classification_report(y_train_balanced, tuned_svc.predict(X_train_balanced)))\n",
    "print(\"----------\")\n",
    "print(\"Classification report on imba data:\")\n",
    "print(classification_report(y_train, tuned_svc.predict(X_train)))\n",
    "print(\"----------\")\n",
    "print(\"Classification report on test data:\")\n",
    "print(classification_report(y_test, tuned_svc.predict(X_test)))\n",
    "print(\"----------\")\n",
    "print(best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
